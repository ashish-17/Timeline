Following are the deliverables for this stage:

\paragraph{General Description}
This project is about setting up an infrastructure for big data analytics using hadoop and demonstrate a data analytics application using event data from various sources like new, twitter etc.

\paragraph{User Type 1}
People interested in analysing data that can't be stored on a single machine.
\begin{itemize}
\item User Interaction Modes: Hive query language.
\item Real World Scenarios:
  \begin{itemize}
  \item Scenario 1 Description: Processing of large server logs.
  \item System Data Input for Scenario 1: Log files.
  \item Input Data Types for Scenario 1: Structures log files.
  \item System Data Output for Scenario 1: Information based on query.
  \item Scenario 2 Description: Text mining.
  \item System Data Input for Scenario 2: Big sources of textual information.
  \item Input Data Types for Scenario 2: Text.
  \item System Data Output for Scenario 2: Information based on query.
  \end{itemize}
\end{itemize}

\paragraph{Project Timeline and Division of Labor}
We will start with studying about hadoop and map reduce systems and then  set up a hadoop system on local host. When we are successful in doing that we will find a small dataset, large enough to fit on a single system and start test analysis over it using hadoop. Then we will repeat the same process by setting up hadoop in pseudo distributed mode. Parallely two team members will start building the demo analytics application over hadoop. In the end we will setup hadoop on multiple clusters and deploy our application over it.
\begin{itemize}
	\item Week 1 and Week 2: Work on requirement gathering and design of system using hadoop.
	\item Week 3 and week 4: Setup hadoop infrastructure over multiple clusters and create an analytics application demonstrating big data analysis using hadoop.
	\item Week 5: Test the application and prepare the necessary documentation & project report.
\end{itemize}